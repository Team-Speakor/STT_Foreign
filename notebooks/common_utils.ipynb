{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"none","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport json\nimport torch\nimport torchaudio\n\ndef get_transcript_from_json(file_stem, label_dir):\n    \"\"\"자막 파일에서 텍스트 추출 함수\"\"\"\n    json_path = os.path.join(label_dir, file_stem + \".json\")\n    if os.path.exists(json_path):\n        with open(json_path, \"r\", encoding=\"utf-8\") as f:\n            data = json.load(f)\n            return data.get(\"transcription\", {}).get(\"AnswerLabelText\", \"\").strip()\n    return None\n\ndef preprocess(batch):\n    \"\"\"오디오 전처리 함수\"\"\"\n    speech_array, sampling_rate = torchaudio.load(batch[\"path\"])\n    resampler = torchaudio.transforms.Resample(orig_freq=sampling_rate, new_freq=16000)\n    batch[\"speech\"] = resampler(speech_array).squeeze().numpy()\n    return batch\n\ndef prepare(batch, processor):\n    \"\"\"프로세서 적용 함수\"\"\"\n    batch[\"input_values\"] = processor(\n        batch[\"speech\"], sampling_rate=16000, return_tensors=\"pt\"\n    ).input_values[0]\n    with processor.as_target_processor():\n        batch[\"labels\"] = processor(batch[\"transcript\"]).input_ids\n    return batch\n\ndef transcribe(audio_path, model, processor):\n    \"\"\"오디오 파일에서 텍스트 추출 함수\"\"\"\n    speech_array, sr = torchaudio.load(audio_path)\n    resampler = torchaudio.transforms.Resample(orig_freq=sr, new_freq=16000)\n    input_values = processor(\n        resampler(speech_array).squeeze().numpy(), \n        return_tensors=\"pt\", \n        sampling_rate=16000\n    ).input_values\n    \n    # GPU 사용 가능 시 GPU로 이동\n    if torch.cuda.is_available():\n        input_values = input_values.to(\"cuda\")\n        model = model.to(\"cuda\")\n    \n    with torch.no_grad():\n        logits = model(input_values).logits\n    predicted_ids = torch.argmax(logits, dim=-1)\n    return processor.decode(predicted_ids[0])","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null}]}