#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
í•œêµ­ì–´ ìŒì„± ì¸ì‹ ëª¨ë¸ í›ˆë ¨ ìŠ¤í¬ë¦½íŠ¸ - Experiment 03: Whisper Turbo Korean
ghost613/whisper-large-v3-turbo-korean ëª¨ë¸ Fine-tuning
"""

import os
import json
import pandas as pd
import numpy as np
import torch
import librosa
import re
from datetime import datetime
from datasets import Dataset, Audio
from transformers import (
    WhisperForConditionalGeneration, 
    WhisperProcessor, 
    WhisperTokenizer,
    Trainer, 
    TrainingArguments, 
    TrainerCallback,
    BitsAndBytesConfig
)
from peft import (
    LoraConfig,
    get_peft_model,
    prepare_model_for_kbit_training
)
from dataclasses import dataclass
from typing import Dict, List, Union, Optional
import matplotlib.pyplot as plt

# ğŸ”§ ë©”ëª¨ë¦¬ ìµœì í™” ë° ë°ë“œë½ í•´ê²° ì„¤ì •
os.environ["NCCL_P2P_DISABLE"] = "1"  # ğŸš¨ í•µì‹¬: NCCL P2P ë¹„í™œì„±í™”
os.environ["CUDA_VISIBLE_DEVICES"] = "1"  # GPU 1 ì‚¬ìš© (GPU 0ì€ í˜„ì¬ ì‹¤í—˜ ì¤‘)
os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True,max_split_size_mb:128"
os.environ["TOKENIZERS_PARALLELISM"] = "false"  # í† í¬ë‚˜ì´ì € ë³‘ë ¬ì²˜ë¦¬ ë¹„í™œì„±í™”
os.environ["OMP_NUM_THREADS"] = "1"  # OpenMP ìŠ¤ë ˆë“œ ìˆ˜ ì œí•œ

def clear_gpu_memory():
    """GPU ë©”ëª¨ë¦¬ ì •ë¦¬ í•¨ìˆ˜"""
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
        torch.cuda.ipc_collect()
        import gc
        gc.collect()

def print_gpu_memory_usage():
    """GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¶œë ¥"""
    if torch.cuda.is_available():
        allocated = torch.cuda.memory_allocated() / 1024**3  # GB
        reserved = torch.cuda.memory_reserved() / 1024**3    # GB
        total = torch.cuda.get_device_properties(0).total_memory / 1024**3  # GB
        print(f"ğŸ’¾ GPU ë©”ëª¨ë¦¬: {allocated:.2f}GB í• ë‹¹ë¨, {reserved:.2f}GB ì˜ˆì•½ë¨, {total:.2f}GB ì´ ìš©ëŸ‰")

def main():
    print("=== ğŸš€ Experiment 03: Whisper Turbo Korean Fine-tuning ===")
    print("ğŸ”§ ghost613/whisper-large-v3-turbo-korean ëª¨ë¸ í•™ìŠµ")
    
    # ì´ˆê¸° ë©”ëª¨ë¦¬ ì •ë¦¬
    clear_gpu_memory()
    
    # GPU ì„¤ì • í™•ì¸
    print(f"CUDA ì‚¬ìš© ê°€ëŠ¥: {torch.cuda.is_available()}")
    print(f"GPU ê°œìˆ˜: {torch.cuda.device_count()}")
    if torch.cuda.is_available():
        print(f"í˜„ì¬ GPU: {torch.cuda.current_device()}")
        print(f"GPU ì´ë¦„: {torch.cuda.get_device_name()}")
        
        # bf16 ì§€ì› ì—¬ë¶€ í™•ì¸
        try:
            bf16_supported = torch.cuda.is_bf16_supported()
            print(f"bf16 ì§€ì›: {bf16_supported}")
        except:
            bf16_supported = False
            print(f"bf16 ì§€ì›: í™•ì¸ ë¶ˆê°€ (Falseë¡œ ì„¤ì •)")
    else:
        bf16_supported = False
    
    # ===== 1. ë°ì´í„° ë¡œë”© =====
    print("\n1. ë°ì´í„° ë¡œë”©...")
    
    train_audio_dir = "../../data/train/audio"
    train_json_dir = "../../data/train/label"
    valid_audio_dir = "../../data/valid/audio" 
    valid_json_dir = "../../data/valid/label"
    
    def load_json_files(directory):
        json_data = []
        for root, dirs, files in os.walk(directory):
            for filename in files:
                if filename.endswith('.json'):
                    with open(os.path.join(root, filename), 'r', encoding='utf-8') as f:
                        data = json.load(f)
                        json_data.append(data)
        return json_data
    
    def create_dataframe(json_data, audio_base_dir):
        data = []
        for item in json_data:
            file_name = item.get('fileName')
            answer_text = item.get('transcription', {}).get('AnswerLabelText', '')
            
            if file_name and answer_text:
                audio_path = None
                for root, dirs, files in os.walk(audio_base_dir):
                    if file_name in files:
                        audio_path = os.path.join(root, file_name)
                        break
                
                if audio_path and os.path.exists(audio_path):
                    data.append({
                        'file_path': audio_path,
                        'text': answer_text
                    })
        
        return pd.DataFrame(data)
    
    train_json = load_json_files(train_json_dir)
    valid_json = load_json_files(valid_json_dir)
    
    train_df = create_dataframe(train_json, train_audio_dir)
    valid_df = create_dataframe(valid_json, valid_audio_dir)
    
    print(f"ğŸ”§ Whisper Turbo Korean: ì „ì²´ ë°ì´í„° ì‚¬ìš© - í•™ìŠµ: {len(train_df)}ê°œ, ê²€ì¦: {len(valid_df)}ê°œ")
    
    # í…ìŠ¤íŠ¸ ì •ê·œí™” (Whisperìš©)
    def prepare_korean_text_for_whisper(text):
        # WhisperëŠ” ë” ë³´ìˆ˜ì ì¸ ì •ê·œí™”ê°€ í•„ìš”
        text = re.sub(r'[^\uAC00-\uD7A3\s.,!?]', '', text)
        text = re.sub(r'\s+', ' ', text).strip()
        return text
    
    train_df['normalized_text'] = train_df['text'].apply(prepare_korean_text_for_whisper)
    valid_df['normalized_text'] = valid_df['text'].apply(prepare_korean_text_for_whisper)
    
    print(f"ìµœì¢… ë°ì´í„° í¬ê¸° - í•™ìŠµ: {len(train_df)}, ê²€ì¦: {len(valid_df)}")
    
    # ===== 2. Whisper ëª¨ë¸ ë¡œë”© =====
    print("\n2. Whisper Turbo Korean ëª¨ë¸ ë¡œë”©...")
    
    MODEL_ID = "ghost613/whisper-large-v3-turbo-korean"
    
    # Processorì™€ Tokenizer ë¡œë”©
    processor = WhisperProcessor.from_pretrained(MODEL_ID)
    tokenizer = WhisperTokenizer.from_pretrained(MODEL_ID)
    
    # 4bit ì–‘ìí™” ì„¤ì • (PEFT í˜¸í™˜)
    quantization_config = BitsAndBytesConfig(
        load_in_4bit=True,
        bnb_4bit_quant_type="nf4",
        bnb_4bit_use_double_quant=True,
        bnb_4bit_compute_dtype=torch.bfloat16,
    )
    
    # ëª¨ë¸ ë¡œë”© (4bit ì–‘ìí™” + PEFT)
    model = WhisperForConditionalGeneration.from_pretrained(
        MODEL_ID,
        quantization_config=quantization_config,
        device_map="auto",
        torch_dtype=torch.bfloat16,
        low_cpu_mem_usage=True,
    )
    
    # ğŸ”§ Whisper íŠ¹í™” ì„¤ì •
    model.config.forced_decoder_ids = None
    model.config.suppress_tokens = []
    model.config.use_cache = False  # í•™ìŠµ ì‹œ ìºì‹œ ë¹„í™œì„±í™”
    
    # ğŸ”§ PEFTë¥¼ ìœ„í•œ ëª¨ë¸ ì¤€ë¹„ (ì–‘ìí™”ëœ ëª¨ë¸)
    model = prepare_model_for_kbit_training(model)
    
    # ğŸ”§ LoRA ì„¤ì • (Whisperìš©) - ì˜¬ë°”ë¥¸ task_type ì‚¬ìš©
    lora_config = LoraConfig(
        r=32,  # LoRA rank - WhisperëŠ” ë” ë†’ì€ rank í•„ìš”
        lora_alpha=64,  # LoRA alpha
        target_modules=[
            "q_proj", "k_proj", "v_proj", "o_proj",  # attention layers
            "fc1", "fc2",  # feed forward layers
        ],
        lora_dropout=0.1,
        bias="none",
        task_type="SEQ_2_SEQ_LM",  # WhisperëŠ” sequence-to-sequence ëª¨ë¸
    )
    
    # ğŸ”§ PEFT ëª¨ë¸ ìƒì„±
    model = get_peft_model(model, lora_config)
    model.print_trainable_parameters()  # í•™ìŠµ ê°€ëŠ¥í•œ íŒŒë¼ë¯¸í„° ìˆ˜ ì¶œë ¥
    
    print("âœ… Whisper Turbo Korean ëª¨ë¸ ë¡œë”© ì™„ë£Œ")
    print(f"ğŸ“Š ëª¨ë¸ íŒŒë¼ë¯¸í„°: {sum(p.numel() for p in model.parameters()):,}")
    print(f"ğŸ“Š í•™ìŠµ ê°€ëŠ¥ íŒŒë¼ë¯¸í„°: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}")
    print_gpu_memory_usage()
    
    # ===== 3. ë°ì´í„°ì…‹ ì¤€ë¹„ =====
    print("\n3. Whisper ë°ì´í„°ì…‹ ì¤€ë¹„...")
    
    def prepare_dataset(df):
        dataset = Dataset.from_pandas(df)
        dataset = dataset.cast_column("file_path", Audio(sampling_rate=16000))
        return dataset
    
    train_dataset = prepare_dataset(train_df)
    valid_dataset = prepare_dataset(valid_df)
    
    def prepare_dataset_for_whisper(batch):
        # ì˜¤ë””ì˜¤ ì²˜ë¦¬
        audio = batch["file_path"]
        array = audio["array"]
        
        # 20ì´ˆ ì œí•œ (OOM ë°©ì§€ë¥¼ ìœ„í•´ 30ì´ˆ â†’ 20ì´ˆ)
        max_samples = 20 * 16000
        if len(array) > max_samples:
            array = array[:max_samples]
        
        # ì •ê·œí™”
        if np.max(np.abs(array)) > 0:
            array = array / np.max(np.abs(array))
        
        # Whisper input features ìƒì„± (ê¸°ë³¸ 30ì´ˆ chunkë¡œ ìë™ íŒ¨ë”©)
        inputs = processor(
            array, 
            sampling_rate=16000,
            return_tensors="pt"
        )
        
        # í…ìŠ¤íŠ¸ í† í°í™” (ìµœì‹  ë°©ì‹)
        labels = processor.tokenizer(
            batch["normalized_text"],
            return_tensors="pt",
            padding=True,
            truncation=True,
            max_length=448  # Whisper í‘œì¤€ ê¸¸ì´
        ).input_ids[0]
        
        # ğŸ”§ Whisper ì „ìš© í˜•ì‹ìœ¼ë¡œ ë°˜í™˜ (input_ids ì œì™¸)
        return {
            "input_features": inputs.input_features[0],
            "labels": labels
        }
    
    print("Whisper ë°ì´í„°ì…‹ ì „ì²˜ë¦¬ ì¤‘...")
    # ğŸ”§ ì»¬ëŸ¼ëª…ì„ ëª…ì‹œì ìœ¼ë¡œ ì§€ì •í•˜ì—¬ ì˜¤ë¥˜ ë°©ì§€
    original_columns = train_dataset.column_names
    train_dataset = train_dataset.map(
        prepare_dataset_for_whisper, 
        remove_columns=original_columns
    )
    valid_dataset = valid_dataset.map(
        prepare_dataset_for_whisper, 
        remove_columns=original_columns
    )
    
    # 30ì´ˆ ì´í•˜ í•„í„°ë§
    def filter_audio_length(example):
        input_features = example["input_features"]
        
        # input_featuresê°€ listì¸ì§€ tensorì¸ì§€ í™•ì¸
        if isinstance(input_features, list):
            # listì¸ ê²½ìš°, ë§ˆì§€ë§‰ ì°¨ì›ì˜ ê¸¸ì´ë¥¼ ê°€ì ¸ì˜´
            audio_length = len(input_features) if len(input_features) > 0 else 0
            if audio_length > 0 and hasattr(input_features[0], '__len__'):
                # 2D listì˜ ê²½ìš° ë§ˆì§€ë§‰ ì°¨ì›
                audio_length = len(input_features[0]) if len(input_features[0]) > 0 else audio_length
        else:
            # tensorì¸ ê²½ìš°
            import torch
            import numpy as np
            if isinstance(input_features, (torch.Tensor, np.ndarray)):
                audio_length = input_features.shape[-1]
            else:
                # ê¸°íƒ€ ê²½ìš° ë³€í™˜ ì‹œë„
                try:
                    if hasattr(input_features, 'shape'):
                        audio_length = input_features.shape[-1]
                    else:
                        audio_length = len(input_features)
                except:
                    audio_length = 0
        
        return audio_length <= 3000  # Whisperì˜ ìµœëŒ€ ê¸¸ì´
    
    train_dataset = train_dataset.filter(filter_audio_length)
    valid_dataset = valid_dataset.filter(filter_audio_length)
    
    print(f"í•„í„°ë§ í›„ - í•™ìŠµ: {len(train_dataset)}, ê²€ì¦: {len(valid_dataset)}")
    print_gpu_memory_usage()
    
    # ===== 4. Whisper ì „ìš© ë°ì´í„° ì½œë ˆì´í„° =====
    @dataclass
    class WhisperDataCollator:
        processor: WhisperProcessor
        
        def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:
            # input_features ë°°ì¹˜ ì²˜ë¦¬
            input_features = [{"input_features": feature["input_features"]} for feature in features]
            batch = self.processor.feature_extractor.pad(input_features, return_tensors="pt")
            
            # labels ë°°ì¹˜ ì²˜ë¦¬
            label_features = [{"input_ids": feature["labels"]} for feature in features]
            labels_batch = self.processor.tokenizer.pad(label_features, return_tensors="pt")
            
            # -100ìœ¼ë¡œ íŒ¨ë”©ëœ ë¶€ë¶„ ë§ˆìŠ¤í‚¹ (ì†ì‹¤ ê³„ì‚°ì—ì„œ ì œì™¸)
            labels = labels_batch["input_ids"].masked_fill(
                labels_batch.attention_mask.ne(1), -100
            )
            
            # BOS í† í°ì´ ìˆë‹¤ë©´ ì œê±° (WhisperëŠ” BOS í† í°ì„ ì˜ˆì¸¡í•˜ì§€ ì•ŠìŒ)
            if (labels[:, 0] == self.processor.tokenizer.bos_token_id).all().cpu().item():
                labels = labels[:, 1:]
            
            batch["labels"] = labels
            
            # ğŸ”§ Whisper + PEFT ë²„ê·¸ ìˆ˜ì •: input_ids ì œê±°
            # WhisperëŠ” input_featuresë§Œ ë°›ê³  input_idsëŠ” ë°›ì§€ ì•ŠìŒ
            if "input_ids" in batch:
                del batch["input_ids"]
            
            return batch
    
    data_collator = WhisperDataCollator(processor=processor)
    
    # ===== 5. Whisper í‰ê°€ ë©”íŠ¸ë¦­ =====
    def compute_whisper_metrics(pred):
        try:
            import evaluate
            wer_metric = evaluate.load("wer")
            
            pred_ids = pred.predictions
            label_ids = pred.label_ids
            
            # -100ì„ íŒ¨ë“œ í† í°ìœ¼ë¡œ ë³€ê²½
            label_ids[label_ids == -100] = tokenizer.pad_token_id
            
            # ë””ì½”ë”©
            pred_str = tokenizer.batch_decode(pred_ids, skip_special_tokens=True)
            label_str = tokenizer.batch_decode(label_ids, skip_special_tokens=True)
            
            # WER ê³„ì‚°
            wer = wer_metric.compute(predictions=pred_str, references=label_str)
            
            return {"wer": wer}
        except Exception as e:
            print(f"ë©”íŠ¸ë¦­ ê³„ì‚° ì˜¤ë¥˜: {e}")
            return {}
    
    # ===== 6. Whisper í•™ìŠµ ì½œë°± =====
    class WhisperTrainingCallback(TrainerCallback):
        """Whisper ì „ìš© í•™ìŠµ ì½œë°±"""
        
        def __init__(self):
            self.best_wer = float('inf')
            self.patience = 0
            self.max_patience = 3
            
        def on_evaluate(self, args, state, control, logs=None, **kwargs):
            if logs is None:
                return
                
            current_wer = logs.get('eval_wer', float('inf'))
            
            if current_wer < self.best_wer:
                self.best_wer = current_wer
                self.patience = 0
                print(f"ğŸ¯ Whisper ìƒˆë¡œìš´ ìµœê³  WER: {current_wer:.4f}")
                
                # Whisper Turbo Korean ëª©í‘œ í™•ì¸
                if current_wer < 0.10:  # ghost613 ëª¨ë¸ì€ 4.89% ë‹¬ì„±
                    print(f"ğŸ‰ ìš°ìˆ˜í•œ ì„±ëŠ¥ ë‹¬ì„±! WER {current_wer:.4f} < 0.10")
                elif current_wer < 0.20:
                    print(f"ğŸ‰ ì¢‹ì€ ì„±ëŠ¥ ë‹¬ì„±! WER {current_wer:.4f} < 0.20")
            else:
                self.patience += 1
                print(f"â³ Early stopping: {self.patience}/{self.max_patience}")
                
            if self.patience >= self.max_patience:
                control.should_training_stop = True
                print("â¹ï¸ Early stopping í™œì„±í™”")
        
        def on_epoch_end(self, args, state, control, **kwargs):
            # ì ê·¹ì ì¸ GPU ë©”ëª¨ë¦¬ ì •ë¦¬
            clear_gpu_memory()
            print(f"ğŸ§¹ ì—í¬í¬ {state.epoch} ì™„ë£Œ, GPU ë©”ëª¨ë¦¬ ì •ë¦¬ë¨")
        
        def on_step_end(self, args, state, control, **kwargs):
            # ë§¤ 50 ìŠ¤í…ë§ˆë‹¤ ë©”ëª¨ë¦¬ ì •ë¦¬
            if state.global_step % 50 == 0:
                clear_gpu_memory()
        
        def on_train_end(self, args, state, control, **kwargs):
            print(f"ğŸ“Š Whisper í•™ìŠµ ì™„ë£Œ - ìµœê³  WER: {self.best_wer:.4f}")
    
    # ===== 7. íŠ¸ë ˆì´ë„ˆ ì„¤ì • =====
    print("\n7. Whisper Turbo Korean íŠ¸ë ˆì´ë„ˆ ì„¤ì •...")
    
    training_args = TrainingArguments(
        output_dir="./whisper-turbo-korean-fine-tuned",
        
        # ğŸ”§ PEFT + 4bit: ê·¹ë„ì˜ ë©”ëª¨ë¦¬ ì ˆì•½
        per_device_train_batch_size=1,      # ìµœì†Œ ë°°ì¹˜ í¬ê¸°
        per_device_eval_batch_size=1,       # ìµœì†Œ ë°°ì¹˜ í¬ê¸°  
        gradient_accumulation_steps=32,     # 16 â†’ 32 (ìœ íš¨ ë°°ì¹˜: 32)
        
        # ğŸ”§ í•™ìŠµ ê¸°ê°„ ì„¤ì •
        num_train_epochs=5,                 # WhisperëŠ” ë¹ ë¥´ê²Œ ìˆ˜ë ´
        max_steps=1500,                     # ì•ˆì „ì¥ì¹˜
        
        # ğŸ”§ í‰ê°€ ë° ì €ì¥ ì£¼ê¸°
        eval_strategy="steps",
        eval_steps=150,                     # ë” ìì£¼ í‰ê°€
        save_steps=300,
        logging_steps=50,
        
        # ğŸ”§ í•™ìŠµë¥  ì„¤ì • (Whisper ê¶Œì¥ì‚¬í•­)
        learning_rate=1e-5,                 # WhisperëŠ” ë‚®ì€ í•™ìŠµë¥  ì‚¬ìš©
        lr_scheduler_type="linear",         # Linear decay
        warmup_steps=500,                   # Warmup ì¤‘ìš”
        weight_decay=0.01,
        
        # ğŸ”§ ì ê·¹ì ì¸ ë©”ëª¨ë¦¬ ìµœì í™” 
        fp16=False,                         # FP16 ì˜¤ë¥˜ ë°©ì§€ë¥¼ ìœ„í•´ ë¹„í™œì„±í™”
        bf16=bf16_supported,                # bf16 ì‚¬ìš© (ì§€ì›í•˜ëŠ” ê²½ìš°ë§Œ)
        dataloader_pin_memory=False,        # ë©”ëª¨ë¦¬ ì ˆì•½
        dataloader_num_workers=0,           # CPU ë©”ëª¨ë¦¬ ì ˆì•½
        gradient_checkpointing=True,        # ë©”ëª¨ë¦¬ ì ˆì•½ (ë°±í”„ë¡­ ì‹œ ë©”ëª¨ë¦¬ ì¬ê³„ì‚°)
        max_grad_norm=1.0,                  # Gradient clipping
        
        # ğŸ”§ ì¶”ê°€ ë©”ëª¨ë¦¬ ìµœì í™” ì„¤ì •
        ddp_find_unused_parameters=False,   # ì‚¬ìš©í•˜ì§€ ì•ŠëŠ” íŒŒë¼ë¯¸í„° ê²€ìƒ‰ ë¹„í™œì„±í™”
        save_safetensors=True,              # ì•ˆì „í•œ í…ì„œ ì €ì¥
        logging_nan_inf_filter=True,        # NaN/Inf í•„í„°ë§ìœ¼ë¡œ ë©”ëª¨ë¦¬ ì ˆì•½
        
        # ğŸ”§ ê¸°íƒ€ ì„¤ì •
        save_total_limit=2,                 # 3 â†’ 2 (ë””ìŠ¤í¬ ì ˆì•½)
        load_best_model_at_end=True,
        metric_for_best_model="wer",
        greater_is_better=False,
        report_to=None,
        disable_tqdm=False,
        group_by_length=False,              # ğŸ”§ Whisperì—ì„œëŠ” input_idsê°€ ì—†ì–´ì„œ ë¹„í™œì„±í™”
        prediction_loss_only=False,
        remove_unused_columns=True,         # ğŸ”§ ë¶ˆí•„ìš”í•œ ì»¬ëŸ¼ ì œê±°í•˜ì—¬ input_ids ì˜¤ë¥˜ ë°©ì§€
        
        # ğŸ”§ Whisper íŠ¹í™” ì„¤ì •
        dataloader_drop_last=True,
        no_cuda=False,
        local_rank=-1,
    )
    
    # ===== 8. ì½œë°± ì„¤ì • =====
    whisper_callback = WhisperTrainingCallback()
    
    trainer = Trainer(
        model=model,
        args=training_args,
        data_collator=data_collator,
        compute_metrics=compute_whisper_metrics,
        train_dataset=train_dataset,
        eval_dataset=valid_dataset,
        processing_class=processor,     # ğŸ”§ ìµœì‹  ë°©ì‹: processing_class ì‚¬ìš©
        callbacks=[whisper_callback]
    )
    
    print("âœ… Whisper Turbo Korean íŠ¸ë ˆì´ë„ˆ ì„¤ì • ì™„ë£Œ")
    print(f"ğŸ”§ Whisper ì„¤ì • (PEFT + LoRA):")
    print(f"  - ëª¨ë¸: ghost613/whisper-large-v3-turbo-korean (4bit + LoRA)")
    print(f"  - ë°°ì¹˜ í¬ê¸°: 1 Ã— 32 = 32 (ìœ íš¨)")
    print(f"  - ì—í¬í¬: 5")
    print(f"  - í•™ìŠµë¥ : 1e-5 (Linear decay)")
    print(f"  - ìµœëŒ€ ì˜¤ë””ì˜¤ ê¸¸ì´: 20ì´ˆ")
    print(f"  - ëª©í‘œ: WER < 0.10 (ìš°ìˆ˜), WER < 0.20 (ì¢‹ìŒ)")
    print(f"  - ì •ë°€ë„: bfloat16 (4bit ì–‘ìí™”)")
    print(f"  - ë©”ëª¨ë¦¬ ìµœì í™”: 4bit ì–‘ìí™” + LoRA + gradient checkpointing")
    
    # ===== 9. Whisper í•™ìŠµ ì‹œì‘ =====
    print("\n9. Whisper Turbo Korean í•™ìŠµ ì‹œì‘...")
    print_gpu_memory_usage()
    
    start_time = datetime.now()
    print(f"ì‹œì‘ ì‹œê°„: {start_time}")
    
    try:
        trainer.train()
        
        end_time = datetime.now()
        duration = end_time - start_time
        print(f"\nâœ… Whisper Turbo Korean í•™ìŠµ ì™„ë£Œ!")
        print(f"ì´ ì‹œê°„: {duration}")
        
        # ğŸ”§ PEFT ëª¨ë¸ ì €ì¥ ê²½ë¡œ (LoRA ì–´ëŒ‘í„°ë§Œ ì €ì¥)
        model_save_path = "../../models/whisper-turbo-korean-lora"
        os.makedirs("../../models", exist_ok=True)
        
        # LoRA ì–´ëŒ‘í„°ë§Œ ì €ì¥ (ì›ë³¸ ëª¨ë¸ì€ ê·¸ëŒ€ë¡œ ìœ ì§€)
        model.save_pretrained(model_save_path)
        processor.save_pretrained(model_save_path)
        print(f"âœ… Whisper LoRA ì–´ëŒ‘í„° ì €ì¥ ì™„ë£Œ: {model_save_path}")
        print(f"ğŸ“Œ ì›ë³¸ ëª¨ë¸: {MODEL_ID}")
        print(f"ğŸ“Œ LoRA ì–´ëŒ‘í„°: {model_save_path}")
        
        # ìµœì¢… í‰ê°€
        eval_results = trainer.evaluate()
        final_wer = eval_results.get('eval_wer', 'N/A')
        best_wer = whisper_callback.best_wer
        
        print(f"ğŸ¯ ìµœì¢… WER: {final_wer:.4f}")
        print(f"ğŸ¯ ìµœê³  WER: {best_wer:.4f}")
        
        # ghost613 ì›ë³¸ ëª¨ë¸ê³¼ ë¹„êµ
        original_wer = 0.0489  # ghost613 ëª¨ë¸ì˜ test WER
        print(f"ğŸ“Š ghost613 ì›ë³¸ WER: {original_wer:.4f}")
        
        if isinstance(best_wer, float):
            if best_wer < 0.05:
                print("ğŸ‰ ghost613 ì›ë³¸ ëª¨ë¸ ìˆ˜ì¤€ ë‹¬ì„±!")
            elif best_wer < 0.10:
                print("ğŸ‰ ìš°ìˆ˜í•œ ì„±ëŠ¥ ë‹¬ì„±! (WER < 10%)")
            elif best_wer < 0.20:
                print("ğŸ‰ ì¢‹ì€ ì„±ëŠ¥ ë‹¬ì„±! (WER < 20%)")
            else:
                print("âš ï¸ ì¶”ê°€ í•™ìŠµ ë˜ëŠ” í•˜ì´í¼íŒŒë¼ë¯¸í„° ì¡°ì • í•„ìš”")
            
            # Wav2Vec2 ëª¨ë¸ë“¤ê³¼ ë¹„êµ
            print(f"\nğŸ“ˆ ì„±ëŠ¥ ë¹„êµ:")
            print(f"  - Whisper Turbo Korean: {best_wer:.4f}")
            print(f"  - Phase 2-1 (Wav2Vec2): 0.6321")
            print(f"  - Phase 2-2.5 (Wav2Vec2): 0.6092")
            
            if best_wer < 0.6092:
                improvement = ((0.6092 - best_wer) / 0.6092) * 100
                print(f"  ğŸš€ ìµœê³  Phase ëŒ€ë¹„ {improvement:.1f}% ê°œì„ !")
        
    except Exception as e:
        print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()
    
    print("\n=== ğŸš€ Whisper Turbo Korean ì‹¤í—˜ ì™„ë£Œ ===")

# ===== 10. ì¶”ë¡  í…ŒìŠ¤íŠ¸ í•¨ìˆ˜ =====
def test_whisper_inference(model_path, audio_file_path):
    """Whisper ëª¨ë¸ ì¶”ë¡  í…ŒìŠ¤íŠ¸"""
    print(f"\nğŸ”§ Whisper ì¶”ë¡  í…ŒìŠ¤íŠ¸: {audio_file_path}")
    
    # ëª¨ë¸ ë¡œë“œ
    model = WhisperForConditionalGeneration.from_pretrained(model_path)
    processor = WhisperProcessor.from_pretrained(model_path)
    
    # ì˜¤ë””ì˜¤ ë¡œë“œ ë° ì²˜ë¦¬
    import librosa
    audio, sr = librosa.load(audio_file_path, sr=16000)
    
    # 30ì´ˆ ì œí•œ
    if len(audio) > 30 * 16000:
        audio = audio[:30 * 16000]
    
    # ì…ë ¥ íŠ¹ì„± ìƒì„±
    input_features = processor(
        audio, 
        sampling_rate=16000, 
        return_tensors="pt"
    ).input_features
    
    # GPUë¡œ ì´ë™ (ì‚¬ìš© ê°€ëŠ¥í•œ ê²½ìš°)
    if torch.cuda.is_available():
        model = model.to("cuda")
        input_features = input_features.to("cuda")
    
    # ì¶”ë¡ 
    with torch.no_grad():
        predicted_ids = model.generate(input_features)
    
    # ë””ì½”ë”©
    transcription = processor.batch_decode(predicted_ids, skip_special_tokens=True)[0]
    
    print(f"ğŸ“ Whisper ì¸ì‹ ê²°ê³¼: {transcription}")
    return transcription

if __name__ == "__main__":
    main()