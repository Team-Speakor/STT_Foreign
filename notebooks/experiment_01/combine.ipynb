{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "85e656d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ace3_yongjae/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "학습 데이터 크기: 1777\n",
      "검증 데이터 크기: 222\n",
      "                                                text  \\\n",
      "0  가장 마지막으로 받은 편지는 두 개 받은 편지는 기억이 안나요 기억이 안 나니까 편...   \n",
      "1  사실 한번 가보고 싶은 나라는 한국이에요 왜냐하면 저는 지금 한국어를 배우고 있고 ...   \n",
      "2  사실은 제가 내일은 딱히 계획은 없어요 아침에 제 제 이 백신 맞으러 갈 거고 어/...   \n",
      "3    네 산책하는 것을 좋아합니다 산책할 때는 공기가 참 좋고 나무들을 보는 게 즐겁습니다   \n",
      "4  드라마와 영화 중에 제가 드라마 더 좋아합니다 왜냐면 드라마는 쪼끔 더 길겠지만 매...   \n",
      "\n",
      "                                     normalized_text  \n",
      "0  가장 마지막으로 받은 편지는 두 개 받은 편지는 기억이 안나요 기억이 안 나니까 편...  \n",
      "1  사실 한번 가보고 싶은 나라는 한국이에요 왜냐하면 저는 지금 한국어를 배우고 있고 ...  \n",
      "2  사실은 제가 내일은 딱히 계획은 없어요 아침에 제 제 이 백신 맞으러 갈 거고 어 ...  \n",
      "3    네 산책하는 것을 좋아합니다 산책할 때는 공기가 참 좋고 나무들을 보는 게 즐겁습니다  \n",
      "4  드라마와 영화 중에 제가 드라마 더 좋아합니다 왜냐면 드라마는 쪼끔 더 길겠지만 매...  \n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import evaluate\n",
    "from datasets import Dataset, Audio, load_dataset\n",
    "from transformers import Wav2Vec2ForCTC, Wav2Vec2Processor, Trainer, TrainingArguments, TrainerCallback\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, List, Optional, Union, Any\n",
    "import re\n",
    "\n",
    "# 메모리 단편화 방지 설정\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "\n",
    "# 데이터 경로 설정\n",
    "train_audio_dir = \"/home/ace3_yongjae/speechRecog/train/training\"\n",
    "train_json_dir = \"/home/ace3_yongjae/speechRecog/train/labeling\"\n",
    "valid_audio_dir = \"/home/ace3_yongjae/speechRecog/valid/validation\" \n",
    "valid_json_dir = \"/home/ace3_yongjae/speechRecog/valid/labeling\"\n",
    "\n",
    "# JSON 파일 로드 함수\n",
    "def load_json_files(directory):\n",
    "    json_data = []\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.endswith('.json'):\n",
    "            with open(os.path.join(directory, filename), 'r', encoding='utf-8') as f:\n",
    "                data = json.load(f)\n",
    "                json_data.append(data)\n",
    "    return json_data\n",
    "\n",
    "# 학습 및 검증 데이터 JSON 로드\n",
    "train_json = load_json_files(train_json_dir)\n",
    "valid_json = load_json_files(valid_json_dir)\n",
    "\n",
    "# 데이터프레임 생성\n",
    "def create_dataframe(json_data, audio_dir):\n",
    "    data = []\n",
    "    for item in json_data:\n",
    "        # JSON에서 필요한 정보 추출\n",
    "        file_name = item.get('fileName')\n",
    "        answer_text = item.get('transcription', {}).get('AnswerLabelText', '')\n",
    "        \n",
    "        # 파일 경로 생성\n",
    "        audio_path = os.path.join(audio_dir, file_name)\n",
    "        \n",
    "        # 데이터가 유효한지 확인 (파일 존재 및 텍스트가 있는지)\n",
    "        if os.path.exists(audio_path) and answer_text:\n",
    "            data.append({\n",
    "                'file_path': audio_path,\n",
    "                'text': answer_text\n",
    "            })\n",
    "    \n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "# 학습 및 검증 데이터프레임 생성\n",
    "train_df = create_dataframe(train_json, train_audio_dir)\n",
    "valid_df = create_dataframe(valid_json, valid_audio_dir)\n",
    "\n",
    "# 라벨 전처리 함수 (공백 포함, 한글+공백만 남김)\n",
    "def prepare_korean_text(text):\n",
    "    # 한글, 공백, 기본 구두점만 남기고 정규화\n",
    "    text = re.sub(r'[^\\uAC00-\\uD7A3\\s.,!?]', '', text)  # Keep punctuation\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "train_df['normalized_text'] = train_df['text'].apply(prepare_korean_text)\n",
    "valid_df['normalized_text'] = valid_df['text'].apply(prepare_korean_text)\n",
    "\n",
    "print(f\"학습 데이터 크기: {len(train_df)}\")\n",
    "print(f\"검증 데이터 크기: {len(valid_df)}\")\n",
    "print(train_df[['text', 'normalized_text']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8f5e6c05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 사전학습 모델의 토크나이저와 프로세서 직접 불러오기\n",
    "MODEL_ID = \"kresnik/wav2vec2-large-xlsr-korean\"\n",
    "processor = Wav2Vec2Processor.from_pretrained(MODEL_ID)\n",
    "\n",
    "# 모델 불러오기 (사전학습된 모델의 토크나이저를 그대로 사용하므로 ignore_mismatched_sizes 필요 없음)\n",
    "model = Wav2Vec2ForCTC.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    ctc_loss_reduction=\"mean\",\n",
    "    pad_token_id=processor.tokenizer.pad_token_id\n",
    ")\n",
    "\n",
    "# 처음에는 feature encoder를 고정\n",
    "model.freeze_feature_encoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f1219795",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'file_path': {'path': '/home/ace3_yongjae/speechRecog/train/training/EN10QC268_EN0171_20211101.wav', 'array': array([0., 0., 0., ..., 0., 0., 0.], shape=(216956,)), 'sampling_rate': 16000}, 'text': '가장 마지막으로 받은 편지는 두 개 받은 편지는 기억이 안나요 기억이 안 나니까 편지 내용도 기억이 안 나요', 'normalized_text': '가장 마지막으로 받은 편지는 두 개 받은 편지는 기억이 안나요 기억이 안 나니까 편지 내용도 기억이 안 나요'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:   0%|          | 0/1777 [00:00<?, ? examples/s]/home/ace3_yongjae/.venv/lib/python3.12/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:174: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "Map: 100%|██████████| 1777/1777 [00:05<00:00, 306.93 examples/s]\n",
      "Map: 100%|██████████| 222/222 [00:00<00:00, 379.59 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_values': Sequence(feature=Value(dtype='float32', id=None), length=-1, id=None), 'labels': Sequence(feature=Value(dtype='int64', id=None), length=-1, id=None)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 데이터셋 준비\n",
    "def prepare_dataset(df):\n",
    "    dataset = Dataset.from_pandas(df)\n",
    "    dataset = dataset.cast_column(\"file_path\", Audio(sampling_rate=16000))\n",
    "    return dataset\n",
    "\n",
    "train_dataset = prepare_dataset(train_df)\n",
    "valid_dataset = prepare_dataset(valid_df)\n",
    "\n",
    "# 샘플 확인\n",
    "print(train_dataset[0])\n",
    "\n",
    "# 데이터 전처리 함수\n",
    "def prepare_dataset_for_model(batch):\n",
    "    # 오디오 로드 및 처리\n",
    "    audio = batch[\"file_path\"]\n",
    "    \n",
    "    # 샘플링 레이트 검증\n",
    "    if audio[\"sampling_rate\"] != 16000:\n",
    "        print(f\"Warning: Expected sampling rate 16000, got {audio['sampling_rate']}\")\n",
    "    \n",
    "    # 오디오 정규화\n",
    "    array = audio[\"array\"]\n",
    "    if np.max(np.abs(array)) > 0:\n",
    "        array = array / np.max(np.abs(array))\n",
    "    \n",
    "    batch[\"input_values\"] = processor(\n",
    "        array, \n",
    "        sampling_rate=16000\n",
    "    ).input_values[0]\n",
    "    \n",
    "    # 텍스트 토큰화 - 기존 모델의 토크나이저 사용\n",
    "    with processor.as_target_processor():\n",
    "        batch[\"labels\"] = processor(batch[\"normalized_text\"]).input_ids\n",
    "    \n",
    "    return batch\n",
    "\n",
    "# 데이터셋에 전처리 함수 적용\n",
    "train_dataset = train_dataset.map(prepare_dataset_for_model, remove_columns=train_dataset.column_names)\n",
    "valid_dataset = valid_dataset.map(prepare_dataset_for_model, remove_columns=valid_dataset.column_names)\n",
    "\n",
    "print(train_dataset.features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "135e6a31",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class DataCollatorCTCWithPadding:\n",
    "    \"\"\"\n",
    "    CTC를 위한 데이터 정렬기\n",
    "    \"\"\"\n",
    "    processor: Wav2Vec2Processor\n",
    "    padding: Union[bool, str] = True\n",
    "\n",
    "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
    "        # 입력 값과 레이블 분리\n",
    "        input_features = [{\"input_values\": feature[\"input_values\"]} for feature in features]\n",
    "        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n",
    "\n",
    "        # 배치 패딩\n",
    "        batch = self.processor.pad(\n",
    "            input_features,\n",
    "            padding=self.padding,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        \n",
    "        # 레이블 패딩\n",
    "        with self.processor.as_target_processor():\n",
    "            labels_batch = self.processor.pad(\n",
    "                label_features,\n",
    "                padding=self.padding,\n",
    "                return_tensors=\"pt\",\n",
    "            )\n",
    "\n",
    "        # 배치에 레이블 추가\n",
    "        batch[\"labels\"] = labels_batch[\"input_ids\"]\n",
    "\n",
    "        return batch\n",
    "\n",
    "# 데이터 정렬기 생성\n",
    "data_collator = DataCollatorCTCWithPadding(processor=processor, padding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "97d91271",
   "metadata": {},
   "outputs": [],
   "source": [
    "# WER과 CER 평가 메트릭 로드\n",
    "wer_metric = evaluate.load(\"wer\")\n",
    "cer_metric = evaluate.load(\"cer\")\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    pred_logits = pred.predictions\n",
    "    pred_ids = np.argmax(pred_logits, axis=-1)\n",
    "    \n",
    "    # -100을 패드 토큰 ID로 변경\n",
    "    pred.label_ids[pred.label_ids == -100] = processor.tokenizer.pad_token_id\n",
    "    \n",
    "    # 예측 및 정답 디코딩\n",
    "    pred_str = processor.batch_decode(pred_ids)\n",
    "    label_str = processor.batch_decode(pred.label_ids, group_tokens=False)\n",
    "    \n",
    "    # WER 및 CER 계산\n",
    "    wer = wer_metric.compute(predictions=pred_str, references=label_str)\n",
    "    cer = cer_metric.compute(predictions=pred_str, references=label_str)\n",
    "    \n",
    "    return {\"wer\": wer, \"cer\": cer}\n",
    "\n",
    "# Feature Encoder를 5에폭 이후에 언프리즈하기 위한 콜백\n",
    "class UnfreezeFeatureEncoderCallback(TrainerCallback):\n",
    "    def on_epoch_begin(self, args, state, control, **kwargs):\n",
    "        if state.epoch == 5:\n",
    "            model = kwargs.get('model', None)\n",
    "            if model is not None:\n",
    "                model.wav2vec2.feature_extractor._freeze_parameters = False\n",
    "                for param in model.wav2vec2.feature_extractor.parameters():\n",
    "                    param.requires_grad = True\n",
    "                print(\"\\n특징 추출기(Feature Encoder)가 언프리즈 되었습니다!\")\n",
    "    \n",
    "    # 에폭 끝에 GPU 캐시 정리\n",
    "    def on_epoch_end(self, args, state, control, **kwargs):\n",
    "        torch.cuda.empty_cache()\n",
    "        print(f\"\\n에폭 {state.epoch} 완료, GPU 캐시 정리됨\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "caadfa37",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_128340/4275728601.py:23: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    }
   ],
   "source": [
    "# 학습 인자 및 Trainer\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./wav2vec2-korean-asr\",\n",
    "    group_by_length=True,\n",
    "    per_device_train_batch_size=2,  # 배치 사이즈 증가\n",
    "    per_device_eval_batch_size=2,   # 배치 사이즈 증가\n",
    "    gradient_accumulation_steps=8,  # 유효 배치 크기 유지\n",
    "    eval_strategy=\"steps\",\n",
    "    num_train_epochs=30,\n",
    "    fp16=True,\n",
    "    save_steps=500,\n",
    "    eval_steps=500,\n",
    "    logging_steps=100,\n",
    "    learning_rate=3e-4,  # 학습률 약간 상향\n",
    "    weight_decay=0.005,\n",
    "    warmup_steps=1000,\n",
    "    save_total_limit=2,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"wer\",\n",
    "    greater_is_better=False,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=valid_dataset,\n",
    "    tokenizer=processor,\n",
    "    callbacks=[UnfreezeFeatureEncoderCallback()]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c78dda56",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ace3_yongjae/.venv/lib/python3.12/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:174: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/ace3_yongjae/.venv/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='810' max='810' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [810/810 1:46:55, Epoch 29/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Wer</th>\n",
       "      <th>Cer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>-0.086200</td>\n",
       "      <td>0.061471</td>\n",
       "      <td>0.406181</td>\n",
       "      <td>0.204557</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "에폭 0.968609865470852 완료, GPU 캐시 정리됨\n",
      "\n",
      "에폭 1.9686098654708521 완료, GPU 캐시 정리됨\n",
      "\n",
      "에폭 2.968609865470852 완료, GPU 캐시 정리됨\n",
      "\n",
      "에폭 3.968609865470852 완료, GPU 캐시 정리됨\n",
      "\n",
      "에폭 4.968609865470852 완료, GPU 캐시 정리됨\n",
      "\n",
      "에폭 5.968609865470852 완료, GPU 캐시 정리됨\n",
      "\n",
      "에폭 6.968609865470852 완료, GPU 캐시 정리됨\n",
      "\n",
      "에폭 7.968609865470852 완료, GPU 캐시 정리됨\n",
      "\n",
      "에폭 8.968609865470851 완료, GPU 캐시 정리됨\n",
      "\n",
      "에폭 9.968609865470851 완료, GPU 캐시 정리됨\n",
      "\n",
      "에폭 10.968609865470851 완료, GPU 캐시 정리됨\n",
      "\n",
      "에폭 11.968609865470851 완료, GPU 캐시 정리됨\n",
      "\n",
      "에폭 12.968609865470851 완료, GPU 캐시 정리됨\n",
      "\n",
      "에폭 13.968609865470851 완료, GPU 캐시 정리됨\n",
      "\n",
      "에폭 14.968609865470851 완료, GPU 캐시 정리됨\n",
      "\n",
      "에폭 15.968609865470851 완료, GPU 캐시 정리됨\n",
      "\n",
      "에폭 16.968609865470853 완료, GPU 캐시 정리됨\n",
      "\n",
      "에폭 17.968609865470853 완료, GPU 캐시 정리됨\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ace3_yongjae/.venv/lib/python3.12/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:174: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/ace3_yongjae/.venv/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "에폭 18.968609865470853 완료, GPU 캐시 정리됨\n",
      "\n",
      "에폭 19.968609865470853 완료, GPU 캐시 정리됨\n",
      "\n",
      "에폭 20.968609865470853 완료, GPU 캐시 정리됨\n",
      "\n",
      "에폭 21.968609865470853 완료, GPU 캐시 정리됨\n",
      "\n",
      "에폭 22.968609865470853 완료, GPU 캐시 정리됨\n",
      "\n",
      "에폭 23.968609865470853 완료, GPU 캐시 정리됨\n",
      "\n",
      "에폭 24.968609865470853 완료, GPU 캐시 정리됨\n",
      "\n",
      "에폭 25.968609865470853 완료, GPU 캐시 정리됨\n",
      "\n",
      "에폭 26.968609865470853 완료, GPU 캐시 정리됨\n",
      "\n",
      "에폭 27.968609865470853 완료, GPU 캐시 정리됨\n",
      "\n",
      "에폭 28.968609865470853 완료, GPU 캐시 정리됨\n",
      "\n",
      "에폭 29.968609865470853 완료, GPU 캐시 정리됨\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ace3_yongjae/.venv/lib/python3.12/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:174: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/ace3_yongjae/.venv/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='28' max='28' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [28/28 00:18]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========== 모델 평가 결과 ==========\n",
      "평가 손실 (eval_loss): 0.0391\n",
      "평가 WER (eval_wer): 0.4066\n",
      "평가 CER (eval_cer): 0.2043\n",
      "평가 런타임 (초): 22.02\n",
      "초당 평가 샘플 수: 10.08\n",
      "초당 평가 스텝 수: 1.27\n",
      "평가 에포크: 29.97\n",
      "====================================\n"
     ]
    }
   ],
   "source": [
    "# 모델 학습 및 저장\n",
    "trainer.train()\n",
    "\n",
    "model.save_pretrained(\"./final-model\")\n",
    "processor.save_pretrained(\"./final-model\")\n",
    "\n",
    "# 모델 평가\n",
    "eval_results = trainer.evaluate(valid_dataset)\n",
    "\n",
    "print(\"\\n========== 모델 평가 결과 ==========\")\n",
    "print(f\"평가 손실 (eval_loss): {eval_results.get('eval_loss'):.4f}\")\n",
    "print(f\"평가 WER (eval_wer): {eval_results.get('eval_wer'):.4f}\")\n",
    "print(f\"평가 CER (eval_cer): {eval_results.get('eval_cer'):.4f}\")\n",
    "print(f\"평가 런타임 (초): {eval_results.get('eval_runtime'):.2f}\")\n",
    "print(f\"초당 평가 샘플 수: {eval_results.get('eval_samples_per_second'):.2f}\")\n",
    "print(f\"초당 평가 스텝 수: {eval_results.get('eval_steps_per_second'):.2f}\")\n",
    "print(f\"평가 에포크: {eval_results.get('epoch'):.2f}\")\n",
    "print(\"====================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b0644b00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델과 프로세서를 함께 저장\n",
    "model.save_pretrained(\"./save-model\")\n",
    "processor.save_pretrained(\"./save-model\")\n",
    "\n",
    "# 추가 메타데이터 저장\n",
    "torch.save({\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'training_args': training_args\n",
    "}, \"./save-model/additional_info.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9987f30e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw pred_ids: tensor([[1204, 1204, 1204, 1204, 1204, 1204, 1204, 1204, 1204, 1204, 1204, 1204,\n",
      "         1204, 1204, 1204, 1204, 1204, 1204, 1204, 1204,  752, 1204, 1204, 1204,\n",
      "         1204, 1204, 1204, 1204, 1204, 1204, 1204, 1204,  167, 1204, 1204, 1204,\n",
      "         1204, 1204, 1204, 1204,  804, 1204, 1204, 1204, 1204, 1204, 1204,  859,\n",
      "          859, 1204, 1204, 1204, 1204,  406, 1204, 1204, 1204, 1204, 1204,  859,\n",
      "          859, 1204, 1204, 1204, 1204, 1204,  459, 1204, 1204, 1204, 1204, 1204,\n",
      "         1204, 1204, 1204, 1204, 1204, 1204, 1204, 1204, 1204, 1204, 1083, 1204,\n",
      "         1204, 1204, 1204, 1204, 1204, 1204,   86, 1204, 1204, 1204, 1204, 1204,\n",
      "          859,  859, 1204, 1204, 1204, 1204, 1204, 1204, 1204, 1204, 1204, 1204,\n",
      "         1204, 1204, 1204, 1204, 1204, 1204, 1204, 1204, 1204, 1204, 1204, 1204,\n",
      "         1204, 1204, 1204, 1204, 1204, 1204, 1204, 1204, 1204, 1204, 1204, 1204,\n",
      "         1204, 1204, 1204, 1204, 1204, 1204, 1204, 1204, 1204, 1204, 1204, 1204,\n",
      "         1204, 1204, 1204, 1204, 1204, 1204, 1204, 1204, 1204, 1204, 1204, 1204,\n",
      "         1204, 1204, 1204,  786, 1204, 1204, 1204, 1204, 1204, 1204, 1204, 1204,\n",
      "         1204, 1204, 1204,  670, 1204, 1204, 1204, 1204, 1204, 1204, 1204, 1204,\n",
      "         1204, 1204, 1204,  804, 1204, 1204, 1204, 1204,  859,  859, 1204, 1204,\n",
      "         1204, 1204, 1204, 1204, 1204, 1204, 1204, 1204,  122, 1204, 1204, 1204,\n",
      "         1204, 1204, 1204, 1204, 1204, 1204, 1204, 1204, 1204, 1204, 1204, 1204,\n",
      "         1204, 1204,  780, 1204, 1204, 1204, 1204, 1204, 1204,  224,  224, 1204,\n",
      "         1204, 1204, 1204, 1204, 1204, 1204, 1204, 1204, 1204, 1204, 1204, 1204,\n",
      "         1204, 1204, 1204, 1204, 1204, 1204, 1204, 1204, 1204, 1204, 1204, 1204,\n",
      "         1204, 1204, 1204, 1204, 1204, 1204, 1204, 1204, 1204, 1204, 1204]],\n",
      "       device='cuda:0')\n",
      "인식 결과: 한국에 안 갔어요 내년에 갖거예\n"
     ]
    }
   ],
   "source": [
    "# 테스트 추론 예제\n",
    "import torch\n",
    "import librosa\n",
    "\n",
    "def transcribe_audio(audio_path, model, processor):\n",
    "    audio, rate = librosa.load(audio_path, sr=16000)\n",
    "    input_values = processor(audio, sampling_rate=rate, return_tensors=\"pt\").input_values\n",
    "    if torch.cuda.is_available():\n",
    "        input_values = input_values.to(\"cuda\")\n",
    "        model = model.to(\"cuda\")\n",
    "    with torch.no_grad():\n",
    "        logits = model(input_values).logits\n",
    "    pred_ids = torch.argmax(logits, dim=-1)\n",
    "    print(f\"Raw pred_ids: {pred_ids}\")\n",
    "    transcription = processor.batch_decode(pred_ids)[0]\n",
    "    return transcription\n",
    "\n",
    "# 테스트 예제\n",
    "test_file = \"/home/ace3_yongjae/speechRecog/valid/validation/EN10QC227_EN0101_20211108.wav\"\n",
    "if os.path.exists(test_file):\n",
    "    transcription = transcribe_audio(test_file, model, processor)\n",
    "    print(f\"인식 결과: {transcription}\")\n",
    "else:\n",
    "    print(f\"테스트 파일을 찾을 수 없습니다: {test_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c68fa629",
   "metadata": {},
   "outputs": [],
   "source": [
    "#나중에 모델 로드 예시\n",
    "model = Wav2Vec2ForCTC.from_pretrained(\"./final-model\")\n",
    "processor = Wav2Vec2Processor.from_pretrained(\"./final-model\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
