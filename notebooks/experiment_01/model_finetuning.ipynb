{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8072c36e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from datasets import Dataset, Audio\n",
    "from transformers import Wav2Vec2ForCTC, Wav2Vec2Processor, Trainer, TrainingArguments, TrainerCallback\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, List, Union\n",
    "\n",
    "# 모델 및 프로세서 불러오기\n",
    "MODEL_ID = \"kresnik/wav2vec2-large-xlsr-korean\"\n",
    "processor = Wav2Vec2Processor.from_pretrained(MODEL_ID)\n",
    "model = Wav2Vec2ForCTC.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    ctc_loss_reduction=\"mean\",\n",
    "    pad_token_id=processor.tokenizer.pad_token_id\n",
    ")\n",
    "model.freeze_feature_encoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0607abf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터셋 준비\n",
    "def prepare_dataset(df):\n",
    "    dataset = Dataset.from_pandas(df)\n",
    "    dataset = dataset.cast_column(\"file_path\", Audio(sampling_rate=16000))\n",
    "    return dataset\n",
    "\n",
    "train_dataset = prepare_dataset(train_df)\n",
    "valid_dataset = prepare_dataset(valid_df)\n",
    "\n",
    "# 데이터 전처리 함수\n",
    "def prepare_dataset_for_model(batch):\n",
    "    audio = batch[\"file_path\"]\n",
    "    array = audio[\"array\"]\n",
    "    if np.max(np.abs(array)) > 0:\n",
    "        array = array / np.max(np.abs(array))\n",
    "    batch[\"input_values\"] = processor(array, sampling_rate=16000).input_values[0]\n",
    "    with processor.as_target_processor():\n",
    "        batch[\"labels\"] = processor(batch[\"normalized_text\"]).input_ids\n",
    "    return batch\n",
    "\n",
    "train_dataset = train_dataset.map(prepare_dataset_for_model, remove_columns=train_dataset.column_names)\n",
    "valid_dataset = valid_dataset.map(prepare_dataset_for_model, remove_columns=valid_dataset.column_names)\n",
    "\n",
    "# 데이터 정렬기\n",
    "@dataclass\n",
    "class DataCollatorCTCWithPadding:\n",
    "    processor: Wav2Vec2Processor\n",
    "    padding: Union[bool, str] = True\n",
    "\n",
    "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
    "        input_features = [{\"input_values\": feature[\"input_values\"]} for feature in features]\n",
    "        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n",
    "        batch = self.processor.pad(input_features, padding=self.padding, return_tensors=\"pt\")\n",
    "        with self.processor.as_target_processor():\n",
    "            labels_batch = self.processor.pad(label_features, padding=self.padding, return_tensors=\"pt\")\n",
    "        batch[\"labels\"] = labels_batch[\"input_ids\"]\n",
    "        return batch\n",
    "\n",
    "data_collator = DataCollatorCTCWithPadding(processor=processor, padding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e03a285",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 평가 메트릭 및 콜백\n",
    "import evaluate\n",
    "wer_metric = evaluate.load(\"wer\")\n",
    "cer_metric = evaluate.load(\"cer\")\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    pred_logits = pred.predictions\n",
    "    pred_ids = np.argmax(pred_logits, axis=-1)\n",
    "    pred.label_ids[pred.label_ids == -100] = processor.tokenizer.pad_token_id\n",
    "    pred_str = processor.batch_decode(pred_ids)\n",
    "    label_str = processor.batch_decode(pred.label_ids, group_tokens=False)\n",
    "    wer = wer_metric.compute(predictions=pred_str, references=label_str)\n",
    "    cer = cer_metric.compute(predictions=pred_str, references=label_str)\n",
    "    return {\"wer\": wer, \"cer\": cer}\n",
    "\n",
    "class UnfreezeFeatureEncoderCallback(TrainerCallback):\n",
    "    def on_epoch_begin(self, args, state, control, **kwargs):\n",
    "        if state.epoch == 5:\n",
    "            model = kwargs.get('model', None)\n",
    "            if model is not None:\n",
    "                model.wav2vec2.feature_extractor._freeze_parameters = False\n",
    "                for param in model.wav2vec2.feature_extractor.parameters():\n",
    "                    param.requires_grad = True\n",
    "                print(\"\\n특징 추출기(Feature Encoder)가 언프리즈 되었습니다!\")\n",
    "    def on_epoch_end(self, args, state, control, **kwargs):\n",
    "        torch.cuda.empty_cache()\n",
    "        print(f\"\\n에폭 {state.epoch} 완료, GPU 캐시 정리됨\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49e875c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습 인자 및 Trainer\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./wav2vec2-korean-asr\",\n",
    "    group_by_length=True,\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=2,\n",
    "    gradient_accumulation_steps=8,\n",
    "    eval_strategy=\"steps\",\n",
    "    num_train_epochs=30,\n",
    "    fp16=True,\n",
    "    save_steps=500,\n",
    "    eval_steps=500,\n",
    "    logging_steps=100,\n",
    "    learning_rate=3e-4,\n",
    "    weight_decay=0.005,\n",
    "    warmup_steps=1000,\n",
    "    save_total_limit=2,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"wer\",\n",
    "    greater_is_better=False,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=valid_dataset,\n",
    "    tokenizer=processor,\n",
    "    callbacks=[UnfreezeFeatureEncoderCallback()]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1650e49f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습 시작 (필요시 주석 해제)\n",
    "# trainer.train()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
