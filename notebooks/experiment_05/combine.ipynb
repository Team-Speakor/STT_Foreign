{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f8ab81be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1,2,3\"\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "052283f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ace3_yongjae/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import evaluate\n",
    "from datasets import Dataset, Audio\n",
    "from transformers import (Wav2Vec2ForCTC, Wav2Vec2Processor, Trainer, TrainingArguments, TrainerCallback)\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, List, Union\n",
    "import re\n",
    "\n",
    "# 경로 설정\n",
    "train_audio_dir = \"/home/ace3_yongjae/speechRecog/train/training\"\n",
    "train_json_dir = \"/home/ace3_yongjae/speechRecog/train/labeling\"\n",
    "valid_audio_dir = \"/home/ace3_yongjae/speechRecog/valid/validation\"\n",
    "valid_json_dir = \"/home/ace3_yongjae/speechRecog/valid/labeling\"\n",
    "\n",
    "# JSON 로드 함수\n",
    "def load_json_files(directory):\n",
    "    return [json.load(open(os.path.join(directory, f), encoding='utf-8'))\n",
    "            for f in os.listdir(directory) if f.endswith('.json')]\n",
    "\n",
    "train_json = load_json_files(train_json_dir)\n",
    "valid_json = load_json_files(valid_json_dir)\n",
    "\n",
    "# DataFrame 생성 (라벨 누락 방지)\n",
    "def create_dataframe(json_data, audio_dir):\n",
    "    data = []\n",
    "    for item in json_data:\n",
    "        file_name = item.get('fileName')\n",
    "        transcription = item.get('transcription', {})\n",
    "        answer_text = transcription.get('AnswerLabelText') or transcription.get('ReadingLabelText', '')\n",
    "        audio_path = os.path.join(audio_dir, file_name)\n",
    "        # 라벨이 비어있거나 None인 경우 제외\n",
    "        if os.path.exists(audio_path) and answer_text and isinstance(answer_text, str) and answer_text.strip():\n",
    "            data.append({'file_path': audio_path, 'text': answer_text})\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "train_df = create_dataframe(train_json, train_audio_dir)\n",
    "valid_df = create_dataframe(valid_json, valid_audio_dir)\n",
    "\n",
    "# 텍스트 정규화\n",
    "def prepare_korean_text(text):\n",
    "    text = re.sub(r'[^\\uAC00-\\uD7A3\\s]', '', text)\n",
    "    return re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "train_df['normalized_text'] = train_df['text'].apply(prepare_korean_text)\n",
    "valid_df['normalized_text'] = valid_df['text'].apply(prepare_korean_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "45934a0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:   0%|          | 0/13773 [00:00<?, ? examples/s]/home/ace3_yongjae/.venv/lib/python3.12/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:174: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "Map: 100%|██████████| 13773/13773 [00:39<00:00, 344.43 examples/s]\n",
      "Map: 100%|██████████| 1723/1723 [00:04<00:00, 394.91 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# 모델 로드\n",
    "MODEL_ID = \"kresnik/wav2vec2-large-xlsr-korean\"\n",
    "processor = Wav2Vec2Processor.from_pretrained(MODEL_ID)\n",
    "model = Wav2Vec2ForCTC.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    ctc_loss_reduction=\"mean\",\n",
    "    pad_token_id=processor.tokenizer.pad_token_id\n",
    ")\n",
    "model.freeze_feature_encoder()\n",
    "\n",
    "# 데이터셋 전처리\n",
    "def prepare_dataset(df):\n",
    "    dataset = Dataset.from_pandas(df)\n",
    "    dataset = dataset.cast_column(\"file_path\", Audio(sampling_rate=16000))\n",
    "    return dataset\n",
    "\n",
    "train_dataset = prepare_dataset(train_df)\n",
    "valid_dataset = prepare_dataset(valid_df)\n",
    "\n",
    "def prepare_dataset_for_model(batch):\n",
    "    audio = batch[\"file_path\"]\n",
    "    array = audio[\"array\"]\n",
    "    if np.max(np.abs(array)) > 0:\n",
    "        array = array / np.max(np.abs(array))\n",
    "    batch[\"input_values\"] = processor(array, sampling_rate=16000).input_values[0]\n",
    "    with processor.as_target_processor():\n",
    "        batch[\"labels\"] = processor(batch[\"normalized_text\"]).input_ids\n",
    "    return batch\n",
    "\n",
    "train_dataset = train_dataset.map(prepare_dataset_for_model, remove_columns=train_dataset.column_names)\n",
    "valid_dataset = valid_dataset.map(prepare_dataset_for_model, remove_columns=valid_dataset.column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3bc87245",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class DataCollatorCTCWithPadding:\n",
    "    processor: Wav2Vec2Processor\n",
    "    padding: Union[bool, str] = True\n",
    "\n",
    "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
    "        input_features = [{\"input_values\": f[\"input_values\"]} for f in features]\n",
    "        label_features = [{\"input_ids\": f[\"labels\"]} for f in features]\n",
    "        batch = self.processor.pad(input_features, padding=self.padding, return_tensors=\"pt\")\n",
    "        with self.processor.as_target_processor():\n",
    "            labels_batch = self.processor.pad(label_features, padding=self.padding, return_tensors=\"pt\")\n",
    "        batch[\"labels\"] = labels_batch[\"input_ids\"]\n",
    "        return batch\n",
    "\n",
    "data_collator = DataCollatorCTCWithPadding(processor=processor, padding=True)\n",
    "\n",
    "# 평가 메트릭\n",
    "wer_metric = evaluate.load(\"wer\")\n",
    "cer_metric = evaluate.load(\"cer\")\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    pred_ids = np.argmax(pred.predictions, axis=-1)\n",
    "    # -100을 pad_token_id로 변환\n",
    "    label_ids = pred.label_ids.copy()\n",
    "    label_ids[label_ids == -100] = processor.tokenizer.pad_token_id\n",
    "    pred_str = processor.batch_decode(pred_ids)\n",
    "    label_str = processor.batch_decode(label_ids, group_tokens=False)\n",
    "    return {\n",
    "        \"wer\": wer_metric.compute(predictions=pred_str, references=label_str),\n",
    "        \"cer\": cer_metric.compute(predictions=pred_str, references=label_str)\n",
    "    }\n",
    "\n",
    "# 메모리 및 Loss 모니터링 콜백\n",
    "class UnfreezeFeatureEncoderCallback(TrainerCallback):\n",
    "    def on_epoch_begin(self, args, state, control, **kwargs):\n",
    "        if state.epoch == 5:\n",
    "            model = kwargs.get('model')\n",
    "            if model is not None:\n",
    "                for param in model.wav2vec2.feature_extractor.parameters():\n",
    "                    param.requires_grad = True\n",
    "                print(\"\\nFeature encoder unfrozen!\")\n",
    "\n",
    "    def on_epoch_end(self, args, state, control, **kwargs):\n",
    "        # Loss 직접 출력\n",
    "        if hasattr(state, 'log_history') and state.log_history:\n",
    "            last_log = state.log_history[-1]\n",
    "            if 'loss' in last_log:\n",
    "                print(f\"Epoch {state.epoch} - Loss: {last_log['loss']:.6f}\")\n",
    "        for i in range(torch.cuda.device_count()):\n",
    "            print(f\"GPU {i} - Allocated: {torch.cuda.memory_allocated(i)/1e9:.2f} GB, Reserved: {torch.cuda.memory_reserved(i)/1e9:.2f} GB\")\n",
    "        torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a76884e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_576469/4280862030.py:25: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    }
   ],
   "source": [
    "# 학습 설정\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./wav2vec2-korean-asr\",\n",
    "    group_by_length=True,\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=2,\n",
    "    gradient_accumulation_steps=8,\n",
    "    eval_strategy=\"epoch\",\n",
    "    num_train_epochs=4,\n",
    "    fp16=False,\n",
    "    eval_accumulation_steps=2,\n",
    "    save_steps=500,\n",
    "    eval_steps=500,\n",
    "    logging_steps=100,\n",
    "    learning_rate=3e-4,\n",
    "    weight_decay=0.005,\n",
    "    warmup_steps=1000,\n",
    "    save_total_limit=2,\n",
    "    load_best_model_at_end=False,\n",
    "    metric_for_best_model=\"wer\",\n",
    "    greater_is_better=False,\n",
    "    ddp_find_unused_parameters=False,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=valid_dataset,\n",
    "    tokenizer=processor,\n",
    "    callbacks=[UnfreezeFeatureEncoderCallback()]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cf8a9e20",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ace3_yongjae/.venv/lib/python3.12/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:174: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/ace3_yongjae/.venv/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1148' max='1148' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1148/1148 1:54:35, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Wer</th>\n",
       "      <th>Cer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.612900</td>\n",
       "      <td>0.196858</td>\n",
       "      <td>0.426433</td>\n",
       "      <td>0.154410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.105800</td>\n",
       "      <td>-0.044596</td>\n",
       "      <td>0.260050</td>\n",
       "      <td>0.090486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>-0.063400</td>\n",
       "      <td>-0.148512</td>\n",
       "      <td>0.203937</td>\n",
       "      <td>0.073290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>-0.157100</td>\n",
       "      <td>-0.229063</td>\n",
       "      <td>0.186573</td>\n",
       "      <td>0.073379</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1.0 - Loss: 0.612900\n",
      "GPU 0 - Allocated: 3.79 GB, Reserved: 14.18 GB\n",
      "GPU 1 - Allocated: 0.02 GB, Reserved: 12.71 GB\n",
      "GPU 2 - Allocated: 0.02 GB, Reserved: 12.23 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ace3_yongjae/.venv/lib/python3.12/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:174: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/ace3_yongjae/.venv/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2.0 - Loss: 0.105800\n",
      "GPU 0 - Allocated: 3.79 GB, Reserved: 15.36 GB\n",
      "GPU 1 - Allocated: 0.02 GB, Reserved: 12.25 GB\n",
      "GPU 2 - Allocated: 0.02 GB, Reserved: 12.10 GB\n",
      "Epoch 3.0 - Loss: -0.063400\n",
      "GPU 0 - Allocated: 3.79 GB, Reserved: 15.86 GB\n",
      "GPU 1 - Allocated: 0.02 GB, Reserved: 12.59 GB\n",
      "GPU 2 - Allocated: 0.02 GB, Reserved: 11.73 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ace3_yongjae/.venv/lib/python3.12/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:174: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/ace3_yongjae/.venv/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4.0 - Loss: -0.157100\n",
      "GPU 0 - Allocated: 3.84 GB, Reserved: 15.84 GB\n",
      "GPU 1 - Allocated: 0.02 GB, Reserved: 12.56 GB\n",
      "GPU 2 - Allocated: 0.02 GB, Reserved: 13.27 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ace3_yongjae/.venv/lib/python3.12/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:174: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/ace3_yongjae/.venv/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 학습\n",
    "trainer.train()\n",
    "\n",
    "# 저장\n",
    "model.save_pretrained(\"./final-model\")\n",
    "processor.save_pretrained(\"./final-model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "41c9395e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch 모델 state_dict가 ./final-model/model_state_dict.pth에 저장완.\n"
     ]
    }
   ],
   "source": [
    "# 모델 .pth 파일 저장\n",
    "pth_save_path = './final-model/model_state_dict.pth'\n",
    "torch.save(model.state_dict(), pth_save_path)\n",
    "\n",
    "print(f\"PyTorch 모델 state_dict가 {pth_save_path}에 저장완.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d91c4d5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "예측 결과: 네 여기 분실물 신고 접수 양식을 작성해 주시겠어요 그리고 지갑의 생김새에 대해 최대한 자세히 작성해 주시면 됩됩니다\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import Wav2Vec2ForCTC, Wav2Vec2Processor\n",
    "import soundfile as sf\n",
    "import numpy as np\n",
    "\n",
    "# 모델과 processor 로드\n",
    "MODEL_ID = \"./final-model\"\n",
    "processor = Wav2Vec2Processor.from_pretrained(MODEL_ID)\n",
    "model = Wav2Vec2ForCTC.from_pretrained(MODEL_ID)\n",
    "model.eval()\n",
    "\n",
    "# .pth 파일에서 state_dict를 불러올 때\n",
    "state_dict = torch.load('./final-model/model_state_dict.pth', map_location='cpu')\n",
    "model.load_state_dict(state_dict)\n",
    "\n",
    "# 테스트할 오디오 파일 경로\n",
    "test_audio_path = \"/home/ace3_yongjae/speechRecog/valid/validation/EN11RC015_EN0208_20211023.wav\"\n",
    "\n",
    "# 오디오 파일 로드 및 전처리\n",
    "audio_input, sample_rate = sf.read(test_audio_path)\n",
    "if sample_rate != 16000:\n",
    "    import librosa\n",
    "    audio_input = librosa.resample(audio_input, orig_sr=sample_rate, target_sr=16000)\n",
    "    sample_rate = 16000\n",
    "\n",
    "# 입력값 정규화\n",
    "if np.max(np.abs(audio_input)) > 0:\n",
    "    audio_input = audio_input / np.max(np.abs(audio_input))\n",
    "\n",
    "# 입력값 추출\n",
    "input_values = processor(audio_input, sampling_rate=16000, return_tensors=\"pt\").input_values\n",
    "\n",
    "# 추론\n",
    "with torch.no_grad():\n",
    "    logits = model(input_values).logits\n",
    "    predicted_ids = torch.argmax(logits, dim=-1)\n",
    "    transcription = processor.batch_decode(predicted_ids)[0]\n",
    "\n",
    "print(\"예측 결과:\", transcription)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
